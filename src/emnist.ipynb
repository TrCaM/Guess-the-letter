{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fix_image(image):\n",
    "  \"\"\"\n",
    "  Edit the original image data so to make them look natural\n",
    "  \"\"\"\n",
    "  image = image.reshape([28, 28])\n",
    "  image = np.fliplr(image)\n",
    "  image = np.rot90(image)\n",
    "  return image\n",
    "\n",
    "def load_data(fix=False, one_hot_label=False, binary=False):\n",
    "  \"\"\"\n",
    "  Loading all the train and test datasets\n",
    "  \"\"\"\n",
    "  train_df = pd.read_csv('./data/emnist-letters-train.csv', header=None)\n",
    "  test_df = pd.read_csv('./data/emnist-letters-test.csv', header=None)\n",
    "\n",
    "  train_X = train_df.iloc[:, 1:].to_numpy()\n",
    "  test_X = test_df.iloc[:, 1:].to_numpy()\n",
    "  if binary:\n",
    "    train_X[train_X > 0] = 1\n",
    "    test_X[test_X > 0] = 1\n",
    "  if fix:\n",
    "    train_X = np.apply_along_axis(fix_image, 1, train_X)\n",
    "    test_X = np.apply_along_axis(fix_image, 1, test_X)\n",
    "\n",
    "  train_y = train_df.iloc[:, 0].to_numpy()\n",
    "  test_y = test_df.iloc[:, 0].to_numpy()\n",
    "  if one_hot_label:\n",
    "    test_y = np.eye(26)[test_y - 1]\n",
    "    train_y = np.eye(26)[train_y - 1]\n",
    "  return (train_X, train_y), (test_X, test_y)\n",
    "\n",
    "def get_letter_from_label(label):\n",
    "  return chr(ord('a') + label - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Investigating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = load_data(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Statistic on classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3396, 'b': 3396, 'c': 3419, 'd': 3398, 'e': 3437, 'f': 3394, 'g': 3385, 'h': 3424, 'i': 3428, 'j': 3402, 'k': 3438, 'l': 3415, 'm': 3402, 'n': 3365, 'o': 3408, 'p': 3430, 'q': 3435, 'r': 3419, 's': 3392, 't': 3436, 'u': 3419, 'v': 3422, 'w': 3423, 'x': 3437, 'y': 3453, 'z': 3427}\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25]), array([3396, 3396, 3419, 3398, 3437, 3394, 3385, 3424, 3428, 3402, 3438,\n",
      "       3415, 3402, 3365, 3408, 3430, 3435, 3419, 3392, 3436, 3419, 3422,\n",
      "       3423, 3437, 3453, 3427]))\n"
     ]
    }
   ],
   "source": [
    "labels, count = np.unique(train_y, return_counts=True)\n",
    "labels = list(map(get_letter_from_label, labels))\n",
    "class_count = dict(zip(labels, count))\n",
    "print(class_count)\n",
    "print(np.unique(train_y-1, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Display examples for a list of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "fig, axs = plt.subplots(5, 6, figsize=(10, 10))\n",
    "for i, (x, y) in enumerate(zip(train_X[:30], train_y[:30])):\n",
    "  ax = axs[i // 6, i % 6]\n",
    "  ax.imshow(x)\n",
    "  ax.set_title(get_letter_from_label(y))\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Introduction\n",
    "TODO: Working on proving the gradient step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "  expX = np.exp(X - np.max(X))\n",
    "  return expX / expX.sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "  def __init__(self, learning_rate, epochs=5, batch_size=256, verbose=True):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.max_iter = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.verbose = verbose\n",
    "    self.losses = []\n",
    "\n",
    "  def fit(self, X, Y, teX, teY):\n",
    "    self.init_weight(X.shape[1], Y.shape[1])\n",
    "    self.init_bias(Y.shape[1])\n",
    "    itr = 0\n",
    "    self.losses = []\n",
    "    for epoch in range(self.max_iter):\n",
    "      for start in range(0, X.shape[0], self.batch_size):\n",
    "        itr += 1\n",
    "        end = min(X.shape[0], start + self.batch_size)\n",
    "        X_b = X[start:end]\n",
    "        Y_b = Y[start:end]\n",
    "        out = self.forward(X_b)\n",
    "        W_grad, b_grad = self.backward(out, X_b, Y_b)\n",
    "        self.W -= self.learning_rate * W_grad\n",
    "        self.b -= self.learning_rate * b_grad\n",
    "        if itr % 100 == 1:\n",
    "          te_out = self.forward(teX)\n",
    "          loss = self.loss(te_out, teY)\n",
    "          labels_pred = np.argmax(te_out, axis=1)\n",
    "          labels = np.argmax(teY, axis=1)\n",
    "          acc = np.sum(labels_pred == labels) / len(labels)\n",
    "          self.losses.append(loss)\n",
    "          if self.verbose:\n",
    "            print(f'Epoch {epoch} - Iter {itr}\\'s loss: {loss:.4f} - Accuracy: {acc:.4f}')\n",
    "    return self\n",
    "\n",
    "  def loss(self, out, Y):\n",
    "    return - np.sum(Y * np.log(out + 1e-8)) / Y.shape[0]\n",
    "\n",
    "  def forward(self, X):\n",
    "    return softmax(X @ self.W + self.b)\n",
    "\n",
    "  def backward(self, out, X, Y):\n",
    "    w_grad = X.T @ (out - Y) / Y.shape[0]\n",
    "    b_grad = np.sum(out - Y, axis=0) / Y.shape[0]\n",
    "    return w_grad, b_grad\n",
    "\n",
    "\n",
    "  def predict(self, X):\n",
    "    return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    return self.forward(X)\n",
    "\n",
    "  def init_weight(self, input_size, output_size):\n",
    "    self.W = np.random.normal(0, 0.01, (input_size, output_size))\n",
    "\n",
    "  def init_bias(self, output_size):\n",
    "    self.b = np.random.normal(0, 0.01, (output_size, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training and Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88800, 784)\n",
      "(88800, 26)\n",
      "(14800, 784)\n",
      "(14800, 26)\n"
     ]
    }
   ],
   "source": [
    "# Reload data which already at their flatten form\n",
    "(train_X, train_y), (test_X, test_y) = load_data(fix=False, one_hot_label=True)\n",
    "# train_X = train_X / 255\n",
    "# test_X = test_X / 255\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Iter 1's loss: 9.6167 - Accuracy: 0.0348\n",
      "Epoch 0 - Iter 101's loss: 9.5334 - Accuracy: 0.1369\n",
      "Epoch 0 - Iter 201's loss: 9.4669 - Accuracy: 0.2386\n",
      "Epoch 0 - Iter 301's loss: 9.4176 - Accuracy: 0.3159\n",
      "Epoch 1 - Iter 401's loss: 9.3856 - Accuracy: 0.3547\n",
      "Epoch 1 - Iter 501's loss: 9.3686 - Accuracy: 0.3761\n",
      "Epoch 1 - Iter 601's loss: 9.3666 - Accuracy: 0.3895\n",
      "Epoch 2 - Iter 701's loss: 9.3784 - Accuracy: 0.3941\n",
      "Epoch 2 - Iter 801's loss: 9.4033 - Accuracy: 0.3952\n",
      "Epoch 2 - Iter 901's loss: 9.4399 - Accuracy: 0.3941\n",
      "Epoch 2 - Iter 1001's loss: 9.4881 - Accuracy: 0.3904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x7fa6a921c310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression(0.001, epochs=3)\n",
    "LR.fit(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 3
   },
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# SVD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = load_data(fix=True, one_hot_label=False)\n",
    "train_y -= 1\n",
    "test_y -= 1\n",
    "train_X = train_X / 255.0\n",
    "test_X = test_X / 255.0\n",
    "\n",
    "# flat =  np.array([np.array(image).flatten('F') for image in train_X])\n",
    "# U, d, Vt = np.linalg.svd(flat[train_y == 0].T)[0]\n",
    "# Uk = U[:, :20]\n",
    "# Uk.shape\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SVDClassifier:\n",
    "  def __init__(self, basis_images=20, verbose=True):\n",
    "    self.verbose = verbose\n",
    "    self.basis_images = basis_images\n",
    "    self.num_classes = 0\n",
    "    self.U_k = None\n",
    "\n",
    "  def fit(self, X, Y):\n",
    "    self.num_classes = len(np.unique(Y))\n",
    "    X = self.transform(X)\n",
    "    if self.verbose:\n",
    "      print(\"Training using SVD decomposition...\")\n",
    "    self.U_k = np.array([\n",
    "        np.linalg.svd(X[Y == label].T)[0][:, :self.basis_images].T\n",
    "        for label in range(self.num_classes)\n",
    "    ])\n",
    "    if self.verbose:\n",
    "      print(\"Finished!\")\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return np.array([np.array(image).flatten('F') for image in X])\n",
    "\n",
    "  def predict(self, X):\n",
    "    X = self.transform(X)\n",
    "    residuals = np.zeros((X.shape[0], self.num_classes))\n",
    "    for i in range(self.num_classes):\n",
    "      U_i = self.U_k[i]\n",
    "      residuals[:, i] = np.linalg.norm((np.identity(X.shape[1]) - U_i.T @ U_i) @ X.T, ord=2, axis=0)\n",
    "    return np.argmin(residuals, axis=1)\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "svd = SVDClassifier()\n",
    "svd.fit(train_X, train_y)\n",
    "svd.U_k.shape\n",
    "y_pred = svd.predict(test_X)\n",
    "np.sum(test_y == y_pred) / len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "  def __init__(self, k=20, verbose=True):\n",
    "    self.verbose = verbose\n",
    "    self.k = k\n",
    "    self.num_classes = 0\n",
    "    self.X = None\n",
    "    self.Y = None\n",
    "\n",
    "  def fit(self, X, Y):\n",
    "    self.num_classes = len(np.unique(Y))\n",
    "    self.X = self.transform(X)\n",
    "    self.Y = Y\n",
    "    if self.verbose:\n",
    "      print(\"Stored all data for predictions!\")\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return np.array([np.array(image).flatten('F') for image in X])\n",
    "\n",
    "  def distances(self, z):\n",
    "    return np.linalg.norm(self.X - z.reshape((1, 784)), ord=2, axis=1)\n",
    "\n",
    "  def find_knn(self, z):\n",
    "    distances = self.distances(z)\n",
    "    return np.argpartition(distances, self.k)[:self.k]\n",
    "\n",
    "  def predict(self, Z):\n",
    "    Z = self.transform(Z)\n",
    "    y_pred = np.zeros(Z.shape[0])\n",
    "    if self.verbose:\n",
    "      print(\"Predicting...\")\n",
    "    for i, z in enumerate(Z):\n",
    "      knn_idx = self.find_knn(z)\n",
    "      knn_labels = self.Y[knn_idx]\n",
    "      y_pred[i] = np.bincount(knn_labels).argmax()\n",
    "      if self.verbose and i % 200 == 0:\n",
    "        print(f'done {i+1} predictions...')\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = load_data(fix=True, one_hot_label=False)\n",
    "train_y -= 1\n",
    "test_y -= 1\n",
    "train_X = train_X / 255.0\n",
    "test_X = test_X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "knn = KNNClassifier()\n",
    "knn.fit(train_X, train_y)\n",
    "y_pred = knn.predict(test_X)\n",
    "np.sum(test_y == y_pred) / len(test_y)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
